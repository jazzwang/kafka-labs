## 實作一

以下可以在 Google Cloud Platform 的 Cloud Shell 中執行

### 使用 Docker 啟動單機版 Kafka Broker

* 複製 Github Repo

<code>
~$ git clone https://github.com/jazzwang/kafka-labs
</code>

* 啟動Kafka Broker 單機版

<code>
~$ cd kafka-labs
~/kafka-labs$ sbin/init.sh
~/kafka-labs$ bin/lab1.sh start
</code>

* 程式碼解說

* 營運觀念： Best Practive of Kafka Cluster
    * **Kafka Broker 最好不要跟 ZooKeeper 節點共構**

* 串流架構設計觀念：
    * 記得考慮網路有可能斷線，務必思考怎麼補資料
    * ![](https://image.slidesharecdn.com/2017-11-12datapipelinematters-171112161249/95/data-pipeline-matters-21-1024.jpg?cb=1510506116)

### 使用 Kafkacat 當作 Producer 與 Consumer

* 列出 Kafka Broker 資訊

<code>
~/kafka-labs$ kafkacat -L -b localhost
</code>

* 傳送資料("abcd")給 Kafka Broker 名為 「${USER} 變數內容」 的 Topic

<code>
~/kafka-labs$ echo "abcd" | kafkacat -P -b localhost -t $USER
</code>

* 從 Kafka Broker 名為 「${USER} 變數內容」 的 Topic 接收資料

<code>
~/kafka-labs$ kafkacat -C -b localhost -t $USER
</code>

* 壓力測試：有兩個 Producer 的狀態
    * 請開兩個 Google Cloud Shell 分頁, 分別執行以下兩段指令
<code>
~/kafka-labs$ for ((i=1;i<=10000;i++)); do echo $i; echo $i | kafkacat -b localhost -t $USER; done
~/kafka-labs$ cat /var/log/syslog | kafkacat -b localhost -t $USER
</code>

--------------------
本文件最後更新於：<script>document.write(document.lastModified);</script>
